{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:35:47.144752Z",
     "start_time": "2019-01-11T13:35:40.880118Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import re,csv, os, itertools, pandas as pd,docx2txt\n",
    "from tqdm import tqdm\n",
    "from pattern.web import PDF\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from spacy.lang.id import Indonesian\n",
    "from html import unescape\n",
    "from unidecode import unidecode\n",
    "from bz2 import BZ2File as bz2\n",
    "from textblob import TextBlob\n",
    "\n",
    "def LoadStopWords(lang):\n",
    "    L = lang.lower().strip()\n",
    "    if L == 'en' or L == 'english' or L == 'inggris':\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stops =  set([t.strip() for t in LoadDocuments(file = './Corpus/stopwords_eng.txt')[0]])\n",
    "    elif L == 'id' or L == 'indonesia' or L=='indonesian':\n",
    "        lemmatizer = Indonesian() \n",
    "        stops = set([t.strip() for t in LoadDocuments(file = './Corpus/stopwords_id.txt')[0]])\n",
    "    else:\n",
    "        print('Warning, language not recognized. Empty StopWords Given')\n",
    "        stops = set(); lemmatizer = None\n",
    "    return stops, lemmatizer\n",
    "\n",
    "def fixTags(T):\n",
    "    getHashtags = re.compile(r\"#(\\w+)\")\n",
    "    pisahtags = re.compile(r'[A-Z][^A-Z]*')\n",
    "    t = T\n",
    "    tagS = re.findall(getHashtags, T)\n",
    "    for tag in tagS:\n",
    "        proper_words = ' '.join(re.findall(pisahtags, tag))\n",
    "        t = t.replace('#'+tag,proper_words)\n",
    "    return t\n",
    "\n",
    "def readBz2(file):\n",
    "    with bz2(file, \"r\") as bzData:\n",
    "        txt = []\n",
    "        for line in bzData:\n",
    "            try:\n",
    "                txt.append(line.strip().decode('utf-8','replace'))\n",
    "            except:\n",
    "                pass\n",
    "    return ' '.join(txt)\n",
    "\n",
    "def LoadDocuments(dPath=None,types=None, file = None): # types = ['pdf','doc','docx','txt','bz2']\n",
    "    Files, Docs = [], []\n",
    "    if types:\n",
    "        for tipe in types:\n",
    "            Files += crawlFiles(dPath,tipe)\n",
    "    if file:\n",
    "        Files = [file]\n",
    "    if not types and not file: # get all files regardless of their extensions\n",
    "        Files += crawlFiles(dPath)\n",
    "    for f in Files:\n",
    "        if f[-3:].lower()=='pdf':\n",
    "            try:\n",
    "                Docs.append(PDF(f).string)\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-3:].lower()=='txt' or f[-3:].lower()=='dic':\n",
    "            try:\n",
    "                df=open(f,\"r\",encoding=\"utf-8\", errors='replace')\n",
    "                Docs.append(df.readlines());df.close()\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-3:].lower()=='bz2':\n",
    "            try:\n",
    "                Docs.append(readBz2(f))\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-4:].lower()=='docx':\n",
    "            try:\n",
    "                Docs.append(docx2txt.process(f))\n",
    "            except:\n",
    "                print('error reading{0}'.format(f))\n",
    "        elif f[-3:].lower()=='csv':\n",
    "            Docs.append(pd.read_csv(f))\n",
    "        else:\n",
    "            print('Unsupported format {0}'.format(f))\n",
    "    if file:\n",
    "        Docs = Docs[0]\n",
    "    return Docs, Files\n",
    "\n",
    "def DelPic(text): #untuk menghilangkan informasi gambar\n",
    "    D = text.split()\n",
    "    D = [d for d in D if 'pic.twitter.com' not in d]\n",
    "    return ' ' .join(D)\n",
    "\n",
    "def LoadSlang(DirSlang):\n",
    "    Slangs =LoadDocuments(file = DirSlang)\n",
    "    SlangDict={}\n",
    "    for slang in Slangs[0]:\n",
    "        try:\n",
    "            key, value = slang.split(':')\n",
    "            SlangDict[key.strip()] = value.strip()\n",
    "        except:\n",
    "            pass\n",
    "    return SlangDict\n",
    "\n",
    "#POS Tagging\n",
    "from nltk.tag import CRFTagger\n",
    "def postag(text):\n",
    "    #Tokenisasi Data\n",
    "    tokenized_sents = word_tokenize(text)\n",
    "    #pemberian Tag tiap token\n",
    "    ct = CRFTagger()\n",
    "    ct.set_model_file('./Corpus/all_indo_man_tag_corpus_model.crf.tagger') \n",
    "    #directorynya disesuaikan meletakan file crfnya, harus download dlu file crfnya\n",
    "    pt = ct.tag(tokenized_sents)\n",
    "    ptN = []\n",
    "    noun = set(['NN','NNP', 'NNS','NNPS'])\n",
    "    tmp = []\n",
    "    for w in pt:\n",
    "        if w[1] in noun:\n",
    "            tmp.append(w[0])\n",
    "    if len(tmp)>0:\n",
    "        ptN.append(' '.join(tmp))\n",
    "    return ' '.join(ptN)\n",
    "\n",
    "\n",
    "def cleanText(T, fix={}, lang = 'id', lemma=None, stops = set(), symbols_remove = False, min_charLen = 0): \n",
    "    # lang & stopS only 2 options : 'en' atau 'id'\n",
    "    # symbols ASCII atau alnum\n",
    "    pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    t = re.sub(pattern,' ',T) #remove urls if any\n",
    "    t = DelPic(t)\n",
    "    t = unescape(t) # html entities fix\n",
    "    t = fixTags(t) # fix abcDef\n",
    "    t = t.lower().strip() # lowercase\n",
    "    t = unidecode(t)\n",
    "    t = ''.join(''.join(s)[:2] for _, s in itertools.groupby(t)) # remove repetition\n",
    "    t = sent_tokenize(t) # sentence segmentation. String to list\n",
    "    for i, K in enumerate(t):\n",
    "        if symbols_remove:\n",
    "            K = re.sub(r'[^.,a-zA-Z0-9 \\n\\.]',' ',K)\n",
    "        \n",
    "        cleanList = []\n",
    "        if lang =='en':\n",
    "            listKata = word_tokenize(K) # word tokenize\n",
    "            for token in listKata:\n",
    "                if token in fix.keys():\n",
    "                    token = fix[token]\n",
    "                if lemma:\n",
    "                    token = lemma.lemmatize(token)\n",
    "                if stops:\n",
    "                    if len(token)>=min_charLen and token not in stops:\n",
    "                        cleanList.append(token)\n",
    "                else:\n",
    "                    if len(token)>=min_charLen:\n",
    "                        cleanList.append(token)\n",
    "            t[i] = ' '.join(cleanList)\n",
    "        else:\n",
    "            if lemma:\n",
    "                K = lemma(K)\n",
    "                listKata = [token.text for token in K]\n",
    "            else:\n",
    "                listKata = TextBlob(K).words\n",
    "                \n",
    "            for token in listKata:\n",
    "                if token in fix.keys():\n",
    "                    token = fix[token]\n",
    "                \n",
    "                if lemma:\n",
    "                    token = lemma(token)[0].lemma_\n",
    "                if stops:    \n",
    "                    if len(token)>=min_charLen and token not in stops:\n",
    "                        cleanList.append(token)\n",
    "                else:\n",
    "                    if len(token)>=min_charLen:\n",
    "                        cleanList.append(token)\n",
    "            t[i] = ' '.join(cleanList)\n",
    "    return ' '.join(t) \n",
    "\n",
    "DaftarSlang = './Corpus/slang.dic'\n",
    "Slangs=LoadSlang(DaftarSlang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:35:47.404366Z",
     "start_time": "2019-01-11T13:35:47.209479Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Username</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Replies</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Language</th>\n",
       "      <th>urlStatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jan 31</td>\n",
       "      <td>@abdullahnurdien</td>\n",
       "      <td>Udah ngeh blm min?\\nItu sms 86mb jam 16.06 tan...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>https://twitter.com/abdullahnurdien/status/958...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jan 31</td>\n",
       "      <td>@abdullahnurdien</td>\n",
       "      <td>Dibaca itu yang kemarin min 86 mb... Pas jam 1...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>https://twitter.com/abdullahnurdien/status/958...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jan 31</td>\n",
       "      <td>@abdullahnurdien</td>\n",
       "      <td>@IndosatCare pusing saya min pake indosat... \\...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>https://twitter.com/abdullahnurdien/status/958...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jan 31</td>\n",
       "      <td>@JasaWirya</td>\n",
       "      <td>Sudah min sudah, lagian apa yang mau di clear ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>https://twitter.com/JasaWirya/status/958414587...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jan 30</td>\n",
       "      <td>@ojieijo</td>\n",
       "      <td>@IndosatCare masi juga lelet euy,, apakah bts ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Indonesian</td>\n",
       "      <td>https://twitter.com/ojieijo/status/95837580477...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Time          Username  \\\n",
       "0  Jan 31  @abdullahnurdien   \n",
       "1  Jan 31  @abdullahnurdien   \n",
       "2  Jan 31  @abdullahnurdien   \n",
       "3  Jan 31        @JasaWirya   \n",
       "4  Jan 30          @ojieijo   \n",
       "\n",
       "                                               Tweet   Replies   Retweets  \\\n",
       "0  Udah ngeh blm min?\\nItu sms 86mb jam 16.06 tan...         1          0   \n",
       "1  Dibaca itu yang kemarin min 86 mb... Pas jam 1...         0          0   \n",
       "2  @IndosatCare pusing saya min pake indosat... \\...         2          0   \n",
       "3  Sudah min sudah, lagian apa yang mau di clear ...         0          0   \n",
       "4  @IndosatCare masi juga lelet euy,, apakah bts ...         1          0   \n",
       "\n",
       "    Likes    Language                                          urlStatus  \n",
       "0       0  Indonesian  https://twitter.com/abdullahnurdien/status/958...  \n",
       "1       0  Indonesian  https://twitter.com/abdullahnurdien/status/958...  \n",
       "2       0  Indonesian  https://twitter.com/abdullahnurdien/status/958...  \n",
       "3       0  Indonesian  https://twitter.com/JasaWirya/status/958414587...  \n",
       "4       0  Indonesian  https://twitter.com/ojieijo/status/95837580477...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fName = './Indosat.csv'\n",
    "df_isat= pd.read_csv(fName)\n",
    "df_isat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:37:05.959701Z",
     "start_time": "2019-01-11T13:36:02.812273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2873ae8250c543a4887cfe60f53bb88c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!!! 4359 tweet\n"
     ]
    }
   ],
   "source": [
    "#Tanpa POS Tag\n",
    "import pickle\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "listTweet = [d for d in df_isat[' Tweet']]\n",
    "\n",
    "stops, lemmatizer = LoadStopWords(lang='id')\n",
    "for i,d in tqdm(enumerate(listTweet)):\n",
    "    listTweet[i] = cleanText(d,Slangs, lemma=lemmatizer,lang='id', stops = stops, symbols_remove = True, min_charLen = 2)\n",
    "print(\"done!!! {0} tweet\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:37:50.750894Z",
     "start_time": "2019-01-11T13:37:50.739898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadar min sms 86 mb jam 16.06 tanggal 30.01 kemarin paketin 16.07 dapet sms gambar komplain sms 2.59 tweet sisa 160 mb dapet sms'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listTweet[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:38:38.722504Z",
     "start_time": "2019-01-11T13:38:10.073517Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06ddcc4cb60a4b2a8a2eb04d30ae1497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "done!!! 4359 tweet\n"
     ]
    }
   ],
   "source": [
    "#Dengan Postag\n",
    "\n",
    "listTweetPos = listTweet\n",
    "\n",
    "stops, lemmatizer = LoadStopWords(lang='id')\n",
    "for i,d in tqdm(enumerate(listTweetPos)):\n",
    "    listTweetPos[i] = postag(d)\n",
    "print(\"done!!! {0} tweet\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:38:44.947517Z",
     "start_time": "2019-01-11T13:38:44.937523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sadar min sms mb jam tanggal kemarin paketin tweet sisa'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listTweetPos[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:38:56.238589Z",
     "start_time": "2019-01-11T13:38:56.225596Z"
    }
   },
   "outputs": [],
   "source": [
    "#save to csv file \n",
    "df_TweetCleanNPT=pd.DataFrame(listTweet,columns=['TweetClean_NonPT'])\n",
    "df_TweetCleanPT=pd.DataFrame(listTweetPos,columns=['TweetClean_PT'])\n",
    "tweetClean=pd.concat([df_isat[' Tweet'],df_TweetCleanNPT, df_TweetCleanPT], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:38:57.360320Z",
     "start_time": "2019-01-11T13:38:57.340329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>TweetClean_NonPT</th>\n",
       "      <th>TweetClean_PT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Udah ngeh blm min?\\nItu sms 86mb jam 16.06 tan...</td>\n",
       "      <td>sadar min sms mb jam tanggal kemarin paketin t...</td>\n",
       "      <td>sadar min sms mb jam tanggal kemarin paketin t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dibaca itu yang kemarin min 86 mb... Pas jam 1...</td>\n",
       "      <td>dibaca kemarin min mb jam paketin</td>\n",
       "      <td>dibaca kemarin min mb jam paketin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@IndosatCare pusing saya min pake indosat... \\...</td>\n",
       "      <td>pusing min kuota</td>\n",
       "      <td>pusing min kuota</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sudah min sudah, lagian apa yang mau di clear ...</td>\n",
       "      <td>min dibilang diuninstal diinstal sebulan min k...</td>\n",
       "      <td>min dibilang diuninstal diinstal sebulan min k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@IndosatCare masi juga lelet euy,, apakah bts ...</td>\n",
       "      <td>masi didaerah karna kantor</td>\n",
       "      <td>masi didaerah karna kantor</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet  \\\n",
       "0  Udah ngeh blm min?\\nItu sms 86mb jam 16.06 tan...   \n",
       "1  Dibaca itu yang kemarin min 86 mb... Pas jam 1...   \n",
       "2  @IndosatCare pusing saya min pake indosat... \\...   \n",
       "3  Sudah min sudah, lagian apa yang mau di clear ...   \n",
       "4  @IndosatCare masi juga lelet euy,, apakah bts ...   \n",
       "\n",
       "                                    TweetClean_NonPT  \\\n",
       "0  sadar min sms mb jam tanggal kemarin paketin t...   \n",
       "1                  dibaca kemarin min mb jam paketin   \n",
       "2                                   pusing min kuota   \n",
       "3  min dibilang diuninstal diinstal sebulan min k...   \n",
       "4                         masi didaerah karna kantor   \n",
       "\n",
       "                                       TweetClean_PT  \n",
       "0  sadar min sms mb jam tanggal kemarin paketin t...  \n",
       "1                  dibaca kemarin min mb jam paketin  \n",
       "2                                   pusing min kuota  \n",
       "3  min dibilang diuninstal diinstal sebulan min k...  \n",
       "4                         masi didaerah karna kantor  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetClean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T13:39:02.479614Z",
     "start_time": "2019-01-11T13:39:02.466622Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataframe_to_csv(filename, DataFrame):\n",
    "    \"\"\"Export entire DataFrame to csv.\"\"\"\n",
    "    output = DataFrame\n",
    "    output.to_csv(filename, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T14:11:54.778020Z",
     "start_time": "2019-01-11T14:11:54.546777Z"
    }
   },
   "outputs": [],
   "source": [
    "#Save Hasil Prepocessing\n",
    "dataframe_to_csv('./TweetClean.csv', tweetClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
